{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b8f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import load_dotenv\n",
    "import os\n",
    "load_dotenv.load_dotenv(\"../../All_LLM_tutorial/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "412fff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59955/2143761620.py:10: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  Settings.llm = Gemini(model_name=\"models/gemini-2.0-flash\")\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setting global parameter\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") # set the embedding model\n",
    "Settings.llm = Gemini(model_name=\"models/gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93587b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=['./articles/2503.19786v1.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f634b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "# global\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.text_splitter = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca924fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, transformations=[text_splitter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1305f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"./embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5580063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./embeddings/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./embeddings/index_store.json.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./embeddings\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb451455",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a knowledgeable and precise assistant specialized in question-answering tasks, \n",
    "particularly from academic and research-based sources. \n",
    "Your goal is to provide accurate, concise, and contextually relevant answers based on the given information.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Comprehension and Accuracy: Carefully read and comprehend the provided context from the research paper to ensure accuracy in your response.\n",
    "Conciseness: Deliver the answer in no more than three sentences, ensuring it is concise and directly addresses the question.\n",
    "Truthfulness: If the context does not provide enough information to answer the question, clearly state, \"I don't know.\"\n",
    "Contextual Relevance: Ensure your answer is well-supported by the retrieved context and does not include any information beyond what is provided.\n",
    "\n",
    "Remember if no context is provided please say you don't know the answer\n",
    "Here is the question and context for you to work with:\n",
    "\n",
    "\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "prompt_tmpl = PromptTemplate(\n",
    "    template=template,\n",
    "    template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa719d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\":prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6eadc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, Gemma 3 is the latest addition to the Gemma family of open language models, designed for text, image, and code. It focuses on adding image understanding and long context while improving multilinguality and STEM-related abilities. The model sizes and architectures are designed to be compatible with standard hardware.\n"
     ]
    }
   ],
   "source": [
    "## Input\n",
    "response = query_engine.query(\"GEMM3에 대한 간단한 설명\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dfa8857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma 3은 10억에서 270억 개의 파라미터로 구성되어 있습니다. Gemma 3 4B 모델은 417M의 임베딩 파라미터와 675M의 비임베딩 파라미터를 가지고 있습니다. Gemma 3 27B 모델은 417M의 임베딩 파라미터와 25,600M의 비임베딩 파라미터를 가지고 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"GEMMA3을 학습시키는데 param 수\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28cdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70863c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6dd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d8d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
