{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cc84d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "import  load_dotenv\n",
    "load_dotenv.load_dotenv(\"../../All_LLM_tutorial/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72afde5",
   "metadata": {},
   "source": [
    "몇가지 컨셉으로 RAG를 다룬다고 한다  \n",
    "1. Loading == Data ingestion: 다양한 데이터 소스로부터 데이터를 얻어냄\n",
    "2. Indexing: Make Vector embedding\n",
    "3. Storing: re-index 없이 바로 저장하기\n",
    "4. Querying: 서브쿼리, 멀티쿼리, Hybrid 쿼리 등 다양한 패턴으로 질문\n",
    "5. Evaluation: query가 얼마나 정확히, 빠르게 대답이 생성되었는지  \n",
    "\n",
    "그 중에 llamaindex는 이 세가지를 중점적으로 다룬다함\n",
    "### 1. Loading\n",
    "### 2. Indexing\n",
    "### 3. Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e3a73",
   "metadata": {},
   "source": [
    "# Loading stage\n",
    "- Document: Data, API output, 어떤 DB로든부터 얻어진 데이터를 보관하는 컨테이너  \n",
    "- Node: 청크를 나타내는 단위, 메타데이터에 추가로 **원본 문서와의 연관을 보관함**->다른 노드연계가능\n",
    "- Reader: 추상적개념 data connector의 구현체, Ingestion을 담당하는듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a95abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "text_list = [\"Hello\", \"World\"]\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d05d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# load documents\n",
    "...\n",
    "\n",
    "# parse nodes\n",
    "parser = SentenceSplitter()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15bb4aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='6f35e2c5-4f27-4bec-a9fd-22a60a3b4644', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='53685ca2-ea73-485e-b01d-5d34fbfaa7da', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='52c87cd40ccfbd7873af4180fced6d38803d4c3684ed60f6513e8d16077e5b8e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Hello', mimetype='text/plain', start_char_idx=0, end_char_idx=5, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='ce766cdc-7d1b-4349-b277-400f396cd6b5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='786aaf15-2a06-452f-a9ae-f4bb6de6b9c3', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='ff5897aa373b6f1f671166b31e806183334dbfd64eb05f8970ca9555a8643e6e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='World', mimetype='text/plain', start_char_idx=0, end_char_idx=5, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1fff227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbe74c",
   "metadata": {},
   "source": [
    "## Inject metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56415efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document(\n",
    "    text=\"text\",\n",
    "    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0efd53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document.metadata = {\"filename\": \"<doc_file_name>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99208efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "filename_fn = lambda filename: {\"file_name\": filename}\n",
    "\n",
    "# automatically sets the metadata of each document according to filename_fn\n",
    "## 이게 뭔...?\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\", file_metadata=filename_fn\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffab82",
   "metadata": {},
   "source": [
    "## ID 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35edf158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/work/enssel_test/yhkim/llamaindex_study/official_document/data/paul_graham_essay.txt', '/home/work/enssel_test/yhkim/llamaindex_study/official_document/data/temp.csv_part_0']\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\", filename_as_id=True).load_data()\n",
    "print([x.doc_id for x in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0038b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99f271",
   "metadata": {},
   "source": [
    "# Nodes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b93b3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter()\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64308cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "\n",
    "# node1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n",
    "# node2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\n",
    "# # set relationships\n",
    "# node1.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(\n",
    "#     node_id=node2.node_id\n",
    "# )\n",
    "# node2.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(\n",
    "#     node_id=node1.node_id\n",
    "# )\n",
    "# nodes = [node1, node2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba26c26",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfd97e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (9) is close to chunk size (25). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=[Document.example()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (9) is close to chunk size (25). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n",
      "/home/work/enssel_test/yhkim/llamaindex_study/.venv/lib/python3.10/site-packages/llama_index/vector_stores/qdrant/base.py:709: UserWarning: Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "  self._client.create_payload_index(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# Ingest directly into a vector db\n",
    "pipeline.run(documents=[Document.example()])\n",
    "\n",
    "# Create your index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from pipeline_storage/llama_cache.\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "pipeline.persist(\"./pipeline_storage\")\n",
    "\n",
    "# load and restore state\n",
    "new_pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "    ],\n",
    ")\n",
    "new_pipeline.load(\"./pipeline_storage\")\n",
    "\n",
    "# will run instantly due to the cache\n",
    "nodes = pipeline.run(documents=[Document.example()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
