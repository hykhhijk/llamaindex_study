{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0b29fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Basic Setting\n",
    "\n",
    "import  load_dotenv\n",
    "load_dotenv.load_dotenv(\"../../All_LLM_tutorial/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3c961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from typing import Dict, List, Any\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class QueryMultiStepEvent(Event):\n",
    "    \"\"\"\n",
    "    Event containing results of a multi-step query process.\n",
    "\n",
    "    Attributes:\n",
    "        nodes (List[NodeWithScore]): List of nodes with their associated scores.\n",
    "        source_nodes (List[NodeWithScore]): List of source nodes with their scores.\n",
    "        final_response_metadata (Dict[str, Any]): Metadata associated with the final response.\n",
    "    \"\"\"\n",
    "\n",
    "    nodes: List[NodeWithScore]\n",
    "    source_nodes: List[NodeWithScore]\n",
    "    final_response_metadata: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9416d98",
   "metadata": {},
   "source": [
    "### Chk' this pattern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이런 방식으로 state의 값을 가져오고 overriding\n",
    "# query = ev.get(\"query\")\n",
    "# await ctx.store.set(\"query\", ev.get(\"query\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f350ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.core.response_synthesizers import (\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "\n",
    "from llama_index.core.schema import QueryBundle, TextNode\n",
    "\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "from typing import cast\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "class MultiStepQueryEngineWorkflow(Workflow):\n",
    "    def combine_queries(\n",
    "        self,\n",
    "        query_bundle: QueryBundle,\n",
    "        prev_reasoning: str,\n",
    "        index_summary: str,\n",
    "        llm: LLM,\n",
    "    ) -> QueryBundle:\n",
    "        \"\"\"Combine queries using StepDecomposeQueryTransform.\"\"\"\n",
    "        transform_metadata = {\n",
    "            \"prev_reasoning\": prev_reasoning,\n",
    "            \"index_summary\": index_summary,\n",
    "        }\n",
    "        return StepDecomposeQueryTransform(llm=llm)(\n",
    "            query_bundle, metadata=transform_metadata\n",
    "        )\n",
    "\n",
    "    def default_stop_fn(self, stop_dict: Dict) -> bool:\n",
    "        \"\"\"Stop function for multi-step query combiner.\"\"\"\n",
    "        query_bundle = cast(QueryBundle, stop_dict.get(\"query_bundle\"))\n",
    "        if query_bundle is None:\n",
    "            raise ValueError(\"Response must be provided to stop function.\")\n",
    "\n",
    "        return \"none\" in query_bundle.query_str.lower()\n",
    "\n",
    "    @step\n",
    "    async def query_multistep(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> QueryMultiStepEvent:\n",
    "        \"\"\"Execute multi-step query process.\"\"\"\n",
    "        prev_reasoning = \"\"\n",
    "        cur_response = None\n",
    "        should_stop = False\n",
    "        cur_steps = 0\n",
    "\n",
    "        # use response\n",
    "        final_response_metadata: Dict[str, Any] = {\"sub_qa\": []}\n",
    "\n",
    "        text_chunks = []\n",
    "        source_nodes = []\n",
    "\n",
    "        query = ev.get(\"query\")\n",
    "        await ctx.store.set(\"query\", ev.get(\"query\"))\n",
    "\n",
    "        llm = Settings.llm\n",
    "        stop_fn = self.default_stop_fn\n",
    "\n",
    "        num_steps = ev.get(\"num_steps\")\n",
    "        query_engine = ev.get(\"query_engine\")\n",
    "        index_summary = ev.get(\"index_summary\")\n",
    "\n",
    "        while not should_stop:\n",
    "            if num_steps is not None and cur_steps >= num_steps:\n",
    "                should_stop = True\n",
    "                break\n",
    "            elif should_stop:\n",
    "                break\n",
    "\n",
    "            updated_query_bundle = self.combine_queries(\n",
    "                QueryBundle(query_str=query),\n",
    "                prev_reasoning,\n",
    "                index_summary,\n",
    "                llm,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Created query for the step - {cur_steps} is: {updated_query_bundle}\"\n",
    "            )\n",
    "\n",
    "            stop_dict = {\"query_bundle\": updated_query_bundle}\n",
    "            if stop_fn(stop_dict):\n",
    "                should_stop = True\n",
    "                break\n",
    "\n",
    "            cur_response = query_engine.query(updated_query_bundle)\n",
    "\n",
    "            # append to response builder\n",
    "            cur_qa_text = (\n",
    "                f\"\\nQuestion: {updated_query_bundle.query_str}\\n\"\n",
    "                f\"Answer: {cur_response!s}\"\n",
    "            )\n",
    "            text_chunks.append(cur_qa_text)\n",
    "            for source_node in cur_response.source_nodes:\n",
    "                source_nodes.append(source_node)\n",
    "            # update metadata\n",
    "            final_response_metadata[\"sub_qa\"].append(\n",
    "                (updated_query_bundle.query_str, cur_response)\n",
    "            )\n",
    "\n",
    "            prev_reasoning += (\n",
    "                f\"- {updated_query_bundle.query_str}\\n\" f\"- {cur_response!s}\\n\"\n",
    "            )\n",
    "            cur_steps += 1\n",
    "\n",
    "        nodes = [\n",
    "            NodeWithScore(node=TextNode(text=text_chunk))\n",
    "            for text_chunk in text_chunks\n",
    "        ]\n",
    "        return QueryMultiStepEvent(\n",
    "            nodes=nodes,\n",
    "            source_nodes=source_nodes,\n",
    "            final_response_metadata=final_response_metadata,\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def synthesize(\n",
    "        self, ctx: Context, ev: QueryMultiStepEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Synthesize the response.\"\"\"\n",
    "        response_synthesizer = get_response_synthesizer()\n",
    "        query = await ctx.store.get(\"query\", default=None)\n",
    "        final_response = await response_synthesizer.asynthesize(\n",
    "            query=query,\n",
    "            nodes=ev.nodes,\n",
    "            additional_source_nodes=ev.source_nodes,\n",
    "        )\n",
    "        final_response.metadata = ev.final_response_metadata\n",
    "\n",
    "        return StopEvent(result=final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7532289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-14 22:10:50--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham_multiq/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham_mu 100%[===================>]  73.28K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2025-07-14 22:10:50 (21.2 MB/s) - ‘data/paul_graham_multiq/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham_multiq/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham_multiq/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60a2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data/paul_graham_multiq/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bbf6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d5aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64e143fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = MultiStepQueryEngineWorkflow(timeout=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06c1a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In which city did the author found his first company, Viaweb?\"\n",
    "\n",
    "# Sets maximum number of steps taken to answer the query.\n",
    "num_steps = 3\n",
    "\n",
    "# Set summary of the index, useful to create modified query at each step.\n",
    "index_summary = \"Used to answer questions about the author\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d01d373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created query for the step - 0 is: In which city was Viaweb founded?\n",
      "Created query for the step - 1 is: In which city was Viaweb founded?\n",
      "Created query for the step - 2 is: In which city was Viaweb founded?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> Question: In which city did the author found his first company, Viaweb?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Answer: Viaweb was founded in Cambridge."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await w.run(\n",
    "    query=query,\n",
    "    query_engine=query_engine,\n",
    "    index_summary=index_summary,\n",
    "    num_steps=num_steps,\n",
    ")\n",
    "\n",
    "# If created query in a step is None, the process will be stopped.\n",
    "\n",
    "display(\n",
    "    Markdown(\"> Question: {}\".format(query)),\n",
    "    Markdown(\"Answer: {}\".format(result)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74d9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
