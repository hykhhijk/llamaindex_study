{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2581721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Basic Setting\n",
    "\n",
    "import  load_dotenv\n",
    "load_dotenv.load_dotenv(\"../../All_LLM_tutorial/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc9b42",
   "metadata": {},
   "source": [
    "Summary, Graph 같은 잡스러운 인덱스가 많은데 전부 프롬프트로 쪼개고 merge하는거 같은데 한국어가 동작할거라 믿을 수 없다  \n",
    "결국은 VectorStoreIndex만 chk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f962c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# Load documents and build index\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\"\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3d25cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (30) is close to chunk size (50). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "Metadata length (24) is close to chunk size (50). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n",
      "Metadata length (25) is close to chunk size (50). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=50, chunk_overlap=0),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75715f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked',\n",
       " \"on, outside of school, were writing and programming. I didn't write essays.\",\n",
       " 'I wrote what beginning writers were supposed to write then, and probably still are: short stories.',\n",
       " 'My stories were awful.',\n",
       " 'They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.get_text() for i in nodes[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8ee13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Exploring Creativity Through Writing and Programming: Crafting Emotionally Rich Short Stories and Awful Tales\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.metadata[\"document_title\"] for i in nodes][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "262e736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (7) is close to chunk size (30). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "# chroma_client = chromadb.EphemeralClient()        ### 휘발성\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_example\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\",\n",
    "    input_files=[\"./data/temp_data.txt\"]\n",
    ").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, \n",
    "    transformations=[SentenceSplitter(chunk_size=30, chunk_overlap=10)],\n",
    "    embed_model=OpenAIEmbedding()\n",
    ")\n",
    "\n",
    "## transformations에 IngestionPipeline을 넣던 SplitterParser를 넣던 동작은 전부 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b645955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='069dedf1-3b4e-4a4c-86c9-ef782044f329', embedding=None, metadata={'file_path': 'data/temp_data.txt', 'file_name': 'temp_data.txt', 'file_type': 'text/plain', 'file_size': 2249, 'creation_date': '2025-07-08', 'last_modified_date': '2025-07-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='bb149176-4d97-4edd-9cdb-be602b821abb', node_type='4', metadata={'file_path': 'data/temp_data.txt', 'file_name': 'temp_data.txt', 'file_type': 'text/plain', 'file_size': 2249, 'creation_date': '2025-07-08', 'last_modified_date': '2025-07-08'}, hash='733bb2504e3bb003974636a5eea9247bbf716b2c8940ea161cbd6443bd4f71ee'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e20ff645-aff0-4330-9128-5ea7db862211', node_type='1', metadata={}, hash='85646a8d9339baf281c5c8a0ccd4cfd9511583c942edbf34a8c6e852a96b13df')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='오늘 오전 서울 중구에서는 대규모 인공지능', mimetype='text/plain', start_char_idx=0, end_char_idx=23, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6391044448661442)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.as_retriever().retrieve(\"오늘 날씨 어때?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f0a2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=TextNode(id_='069dedf1-3b4e-4a4c-86c9-ef782044f329', embedding=None, metadata={'file_path': 'data/temp_data.txt', 'file_name': 'temp_data.txt', 'file_type': 'text/plain', 'file_size': 2249, 'creation_date': '2025-07-08', 'last_modified_date': '2025-07-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='bb149176-4d97-4edd-9cdb-be602b821abb', node_type='4', metadata={'file_path': 'data/temp_data.txt', 'file_name': 'temp_data.txt', 'file_type': 'text/plain', 'file_size': 2249, 'creation_date': '2025-07-08', 'last_modified_date': '2025-07-08'}, hash='733bb2504e3bb003974636a5eea9247bbf716b2c8940ea161cbd6443bd4f71ee'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e20ff645-aff0-4330-9128-5ea7db862211', node_type='1', metadata={}, hash='85646a8d9339baf281c5c8a0ccd4cfd9511583c942edbf34a8c6e852a96b13df')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='오늘 오전 서울 중구에서는 대규모 인공지능', mimetype='text/plain', start_char_idx=0, end_char_idx=23, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6391044448661442)]\n"
     ]
    }
   ],
   "source": [
    "# get collection\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_example\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# create a query engine\n",
    "query_engine = index.as_retriever()\n",
    "response = query_engine.retrieve(\"오늘 날씨 어때?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52cdf5e",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c0f64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_update = chroma_collection.get(limit=1)\n",
    "doc_to_update[\"metadatas\"][0] = {\n",
    "    **doc_to_update[\"metadatas\"][0],\n",
    "    **{\"author\": \"Paul Graham\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acf56dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection.update(\n",
    "    ids=[doc_to_update[\"ids\"][0]], metadatas=[doc_to_update[\"metadatas\"][0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35a5c470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'creation_date': '2025-07-08', '_node_type': 'TextNode', 'file_name': 'temp_data.txt', 'file_path': 'data/temp_data.txt', 'doc_id': '6b331ad6-231f-4c2b-affa-96a654514218', 'file_type': 'text/plain', 'last_modified_date': '2025-07-08', '_node_content': '{\"id_\": \"7034935c-8ffe-43fa-b500-73facc33e6fa\", \"embedding\": null, \"metadata\": {\"file_path\": \"data/temp_data.txt\", \"file_name\": \"temp_data.txt\", \"file_type\": \"text/plain\", \"file_size\": 2249, \"creation_date\": \"2025-07-08\", \"last_modified_date\": \"2025-07-08\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"6b331ad6-231f-4c2b-affa-96a654514218\", \"node_type\": \"4\", \"metadata\": {\"file_path\": \"data/temp_data.txt\", \"file_name\": \"temp_data.txt\", \"file_type\": \"text/plain\", \"file_size\": 2249, \"creation_date\": \"2025-07-08\", \"last_modified_date\": \"2025-07-08\"}, \"hash\": \"733bb2504e3bb003974636a5eea9247bbf716b2c8940ea161cbd6443bd4f71ee\", \"class_name\": \"RelatedNodeInfo\"}}, \"metadata_template\": \"{key}: {value}\", \"metadata_separator\": \"\\\\n\", \"text\": \"\", \"mimetype\": \"text/plain\", \"start_char_idx\": 0, \"end_char_idx\": 927, \"metadata_seperator\": \"\\\\n\", \"text_template\": \"{metadata_str}\\\\n\\\\n{content}\", \"class_name\": \"TextNode\"}', 'ref_doc_id': '6b331ad6-231f-4c2b-affa-96a654514218', 'file_size': 2249, 'author': 'Paul Graham', 'document_id': '6b331ad6-231f-4c2b-affa-96a654514218'}\n"
     ]
    }
   ],
   "source": [
    "updated_doc = chroma_collection.get(limit=1)\n",
    "print(updated_doc[\"metadatas\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da7c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca14459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e325202a",
   "metadata": {},
   "source": [
    "### Metadata insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d49ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import BaseExtractor\n",
    "\n",
    "\n",
    "class CustomExtractor(BaseExtractor):\n",
    "    async def aextract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\": i\n",
    "            }\n",
    "            for i, node in enumerate(nodes)\n",
    "        ]\n",
    "        return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c8f2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [\n",
    "    SentenceSplitter(),\n",
    "    CustomExtractor(),\n",
    "    OpenAIEmbedding(),\n",
    "\n",
    "]\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=transformations\n",
    ")\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69b6bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_path': 'data/temp_data.txt',\n",
       "  'file_name': 'temp_data.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 2249,\n",
       "  'creation_date': '2025-07-08',\n",
       "  'last_modified_date': '2025-07-08',\n",
       "  'custom': 0}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.metadata for i in nodes[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8fa133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b179c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8791429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf63aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e41945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f26dd9c0",
   "metadata": {},
   "source": [
    "## General pattern  \n",
    "IngestionPipeline은 전처리 파이프라인 느낌, 따라서 index가 아닌 node를 반환(node, index는 저장 여부의 차이임)  \n",
    "VectorStore는 여기서 저장도 붙여 index 생성 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7da47d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "# global\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "# per-index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9269f548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length (9) is close to chunk size (25). Resulting chunks are less than 50 tokens. Consider increasing the chunk size or decreasing the size of your metadata to avoid this.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n",
    "        # TitleExtractor(),\n",
    "        OpenAIEmbedding(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=[Document.example()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
