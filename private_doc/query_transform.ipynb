{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738e83c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Basic Setting\n",
    "\n",
    "import  load_dotenv\n",
    "load_dotenv.load_dotenv(\"../../All_LLM_tutorial/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860fe014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/enssel_test/yhkim/llamaindex_study/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads https://huggingface.co/BAAI/bge-small-en-v1.5\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "Settings.embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fd47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_example\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"cloud\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14f1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53a5db",
   "metadata": {},
   "source": [
    "# Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d66632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\n",
    "from llama_index.core.selectors import (\n",
    "    PydanticMultiSelector,\n",
    "    PydanticSingleSelector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dcbdaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic selectors feed in pydantic objects to a function calling API\n",
    "# single selector (pydantic, function calling)\n",
    "# selector = PydanticSingleSelector.from_defaults()\n",
    "\n",
    "# multi selector (pydantic, function calling)\n",
    "# selector = PydanticMultiSelector.from_defaults()\n",
    "\n",
    "# LLM selectors use text completion endpoints\n",
    "# single selector (LLM)\n",
    "# selector = LLMSingleSelector.from_defaults()\n",
    "# multi selector (LLM)\n",
    "selector = LLMMultiSelector.from_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b47bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import ToolMetadata\n",
    "\n",
    "tool_choices = [\n",
    "    ToolMetadata(\n",
    "        name=\"covid_nyt\",\n",
    "        description=(\"This tool contains a NYT news article about COVID-19\"),\n",
    "    ),\n",
    "    ToolMetadata(\n",
    "        name=\"covid_wiki\",\n",
    "        description=(\"This tool contains the Wikipedia page about COVID-19\"),\n",
    "    ),\n",
    "    ToolMetadata(\n",
    "        name=\"covid_tesla\",\n",
    "        description=(\"This tool contains the Wikipedia page about apples\"),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "538461fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: prompt<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some choices are given below. It is provided in a numbered list (1 to {num_choices}), where each item in the list corresponds to a summary.\n",
      "---------------------\n",
      "{context_list}\n",
      "---------------------\n",
      "Using only the choices above and not prior knowledge, return the top choices (no more than {max_outputs}, but only select what is needed) that are most relevant to the question: '{query_str}'\n",
      "\n",
      "\n",
      "The output should be ONLY JSON formatted as a JSON instance.\n",
      "\n",
      "Here is an example:\n",
      "[\n",
      "    {{\n",
      "        choice: 1,\n",
      "        reason: \"<insert reason for choice>\"\n",
      "    }},\n",
      "    ...\n",
      "]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(selector.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04447e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_result = selector.select(\n",
    "    tool_choices, query=\"Tell me more about COVID-19\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "388027f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SingleSelection(index=0, reason='This choice contains a news article about COVID-19, providing current information and updates.'),\n",
       " SingleSelection(index=1, reason='This choice contains the Wikipedia page about COVID-19, which offers comprehensive information and background on the topic.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_result.selections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69ba9a",
   "metadata": {},
   "source": [
    "# Query Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6469eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "query_gen_str = \"\"\"\\\n",
    "You are a helpful assistant that generates multiple search queries based on a \\\n",
    "single input query. Generate {num_queries} search queries, one on each line, \\\n",
    "related to the following input query:\n",
    "Query: {query}\n",
    "Queries:\n",
    "\"\"\"\n",
    "query_gen_prompt = PromptTemplate(query_gen_str)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "def generate_queries(query: str, llm, num_queries: int = 4):\n",
    "    response = llm.predict(\n",
    "        query_gen_prompt, num_queries=num_queries, query=query\n",
    "    )\n",
    "    # assume LLM proper put each query on a newline\n",
    "    queries = response.split(\"\\n\")\n",
    "    queries_str = \"\\n\".join(queries)\n",
    "    print(f\"Generated queries:\\n{queries_str}\")\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b11544bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. 퇴근하는 방법과 팁\n",
      "2. 지금 퇴근할 수 있는 방법\n",
      "3. 퇴근 시간 단축하는 방법\n",
      "4. 퇴근 후 할 일 추천\n"
     ]
    }
   ],
   "source": [
    "queries = generate_queries(\"지금 당장 퇴근하는 방법은?\", llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d164d00",
   "metadata": {},
   "source": [
    "## Query Rewriting (using QueryTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28258b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84c3d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "query_bundle = hyde.run(\"RAG 기술 설명해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b705c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAG( Retrieval-Augmented Generation) 기술은 자연어 처리(NLP) 분야에서 정보 검색과 생성 모델을 결합한 혁신적인 접근 방식입니다. 이 기술은 주로 대규모 언어 모델(예: GPT-3, BERT 등)과 정보 검색 시스템을 통합하여, 사용자가 질문을 했을 때 보다 정확하고 관련성 높은 답변을 제공하는 데 초점을 맞추고 있습니다.\\n\\nRAG의 작동 방식은 크게 두 단계로 나눌 수 있습니다. 첫 번째 단계는 정보 검색 단계로, 사용자의 질문에 대한 관련 문서나 정보를 데이터베이스에서 검색하는 것입니다. 이 과정에서는 일반적으로 TF-IDF, BM25와 같은 전통적인 정보 검색 기법이나, 최근에는 딥러닝 기반의 검색 모델이 사용됩니다. 검색된 문서들은 사용자의 질문과 관련된 내용을 포함하고 있어야 합니다.\\n\\n두 번째 단계는 생성 단계로, 검색된 정보를 바탕으로 자연어 생성 모델이 최종 답변을 생성하는 것입니다. 이 단계에서는 검색된 문서의 내용을 요약하거나, 질문에 대한 구체적인 답변을 생성하는 데 사용됩니다. RAG 모델은 이러한 두 단계를 통합하여, 정보의 정확성과 생성의 유창성을 동시에 확보할 수 있습니다.\\n\\nRAG 기술의 장점은 정보의 신뢰성을 높이고, 사용자가 원하는 정보를 보다 정확하게 제공할 수 있다는 점입니다. 또한, 대규모 데이터셋에서 학습된 언어 모델이기 때문에, 다양한 주제에 대해 유연하게 대응할 수 있는 능력을 가지고 있습니다. 이러한 특성 덕분에 RAG는 고객 지원, 질의응답 시스템, 콘텐츠 생성 등 다양한 분야에서 활용되고 있습니다. \\n\\n결론적으로, RAG 기술은 정보 검색과 자연어 생성을 결합하여, 보다 효과적이고 정확한 정보 제공을 가능하게 하는 혁신적인 방법론으로 자리잡고 있습니다.',\n",
       " 'RAG 기술 설명해줘']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_bundle.custom_embedding_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7f00420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.question_gen import LLMQuestionGenerator\n",
    "from llama_index.question_gen.openai import OpenAIQuestionGenerator\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b748882",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "question_gen = OpenAIQuestionGenerator.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52eb40a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: question_gen_prompt<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a world class state of the art agent.\n",
      "\n",
      "You have access to multiple tools, each representing a different data source or API.\n",
      "Each of the tools has a name and a description, formatted as a JSON dictionary.\n",
      "The keys of the dictionary are the names of the tools and the values are the descriptions.\n",
      "Your purpose is to help answer a complex user question by generating a list of sub questions that can be answered by the tools.\n",
      "\n",
      "These are the guidelines you consider when completing your task:\n",
      "* Be as specific as possible\n",
      "* The sub questions should be relevant to the user question\n",
      "* The sub questions should be answerable by the tools provided\n",
      "* You can generate multiple sub questions for each tool\n",
      "* Tools must be specified by their name, not their description\n",
      "* You don't need to use a tool if you don't think it's relevant\n",
      "\n",
      "Output the list of sub questions by calling the SubQuestionList function.\n",
      "\n",
      "## Tools\n",
      "```json\n",
      "{tools_str}\n",
      "```\n",
      "\n",
      "## User Question\n",
      "{query_str}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(question_gen.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "674b63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import ToolMetadata\n",
    "\n",
    "tool_choices = [\n",
    "    ToolMetadata(\n",
    "        name=\"uber_2021_10k\",\n",
    "        description=(\n",
    "            \"Provides information about Uber financials for year 2021\"\n",
    "        ),\n",
    "    ),\n",
    "    ToolMetadata(\n",
    "        name=\"lyft_2021_10k\",\n",
    "        description=(\n",
    "            \"Provides information about Lyft financials for year 2021\"\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68dfe9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "\n",
    "query_str = \"Compare and contrast Uber and Lyft\"\n",
    "choices = question_gen.generate(tool_choices, QueryBundle(query_str=query_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8d02baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SubQuestion(sub_question='What were the total revenues for Uber in 2021?', tool_name='uber_2021_10k'),\n",
       " SubQuestion(sub_question='What were the total revenues for Lyft in 2021?', tool_name='lyft_2021_10k'),\n",
       " SubQuestion(sub_question='What were the net profits for Uber in 2021?', tool_name='uber_2021_10k'),\n",
       " SubQuestion(sub_question='What were the net profits for Lyft in 2021?', tool_name='lyft_2021_10k')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b389427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActChatFormatter\n",
    "from llama_index.core.agent.react.output_parser import ReActOutputParser\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "948defe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(sql: str) -> str:\n",
    "    \"\"\"Given a SQL input string, execute it.\"\"\"\n",
    "    # NOTE: This is a mock function\n",
    "    return f\"Executed {sql}\"\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "tool1 = FunctionTool.from_defaults(fn=execute_sql)\n",
    "tool2 = FunctionTool.from_defaults(fn=add)\n",
    "tools = [tool1, tool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caad23bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\\n\\n## Tools\\n\\nYou have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\\nThis may require breaking the task into subtasks and using different tools to complete each subtask.\\n\\nYou have access to the following tools:\\n> Tool Name: execute_sql\\nTool Description: execute_sql(sql: str) -> str\\nGiven a SQL input string, execute it.\\nTool Args: {\"properties\": {\"sql\": {\"title\": \"Sql\", \"type\": \"string\"}}, \"required\": [\"sql\"], \"type\": \"object\"}\\n\\n> Tool Name: add\\nTool Description: add(a: int, b: int) -> int\\nAdd two numbers.\\nTool Args: {\"properties\": {\"a\": {\"title\": \"A\", \"type\": \"integer\"}, \"b\": {\"title\": \"B\", \"type\": \"integer\"}}, \"required\": [\"a\", \"b\"], \"type\": \"object\"}\\n\\n\\n\\n## Output Format\\n\\nPlease answer in the same language as the question and use the following format:\\n\\n```\\nThought: The current language of the user is: (user\\'s language). I need to use a tool to help me answer the question.\\nAction: tool name (one of execute_sql, add) if using a tool.\\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\\n```\\n\\nPlease ALWAYS start with a Thought.\\n\\nNEVER surround your response with markdown code markers. You may use code markers within your response if you need to.\\n\\nPlease use a valid JSON format for the Action Input. Do NOT do this {\\'input\\': \\'hello world\\', \\'num_beams\\': 5}. If you include the \"Action:\" line, then you MUST include the \"Action Input:\" line too, even if the tool does not need kwargs, in that case you MUST use \"Action Input: {}\".\\n\\nIf this format is used, the tool will respond in the following format:\\n\\n```\\nObservation: tool response\\n```\\n\\nYou should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in one of the following two formats:\\n\\n```\\nThought: I can answer without using any more tools. I\\'ll use the user\\'s language to answer\\nAnswer: [your answer here (In the same language as the user\\'s question)]\\n```\\n\\n```\\nThought: I cannot answer the question with the provided tools.\\nAnswer: [your answer here (In the same language as the user\\'s question)]\\n```\\n\\n## Current Conversation\\n\\nBelow is the current conversation consisting of interleaving human and assistant messages.\\n')]),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Can you find the top three rows from the table named `revenue_years`')])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_formatter = ReActChatFormatter()\n",
    "output_parser = ReActOutputParser()\n",
    "input_msgs = chat_formatter.format(\n",
    "    tools,\n",
    "    [\n",
    "        ChatMessage(\n",
    "            content=\"Can you find the top three rows from the table named `revenue_years`\",\n",
    "            role=\"user\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "input_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915372f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
