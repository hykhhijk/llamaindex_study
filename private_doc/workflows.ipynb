{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f94cf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Basic Setting\n",
    "\n",
    "import  load_dotenv\n",
    "load_dotenv.load_dotenv(\"../../All_LLM_tutorial/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f36ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/enssel_test/yhkim/llamaindex_study/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import chromadb\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedfbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_example\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"cloud\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eaf9b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis:\n",
      "This joke plays on the pun of \"fish and ships\" sounding like \"fish and chips,\" a popular dish at seafood restaurants. The joke also incorporates the pirate theme by mentioning a pirate going to a seafood restaurant, which adds an element of humor.\n",
      "\n",
      "Critique:\n",
      "Overall, this joke is light-hearted and playful, making it suitable for a general audience. The pun is clever and well-executed, adding an element of surprise and humor. However, the joke may be considered somewhat predictable as the punchline is somewhat expected once the setup is established. Additionally, the humor may not be particularly sophisticated or nuanced, which could limit its appeal to certain audiences. Overall, while this joke may not be groundbreaking or particularly original, it is still a fun and enjoyable play on words.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "# `pip install llama-index-llms-openai` if you don't already have it\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "class JokeEvent(Event):\n",
    "    joke: str\n",
    "\n",
    "\n",
    "class JokeFlow(Workflow):\n",
    "    llm = OpenAI()\n",
    "\n",
    "    @step\n",
    "    async def generate_joke(self, ev: StartEvent) -> JokeEvent:\n",
    "        topic = ev.topic\n",
    "\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "        return JokeEvent(joke=str(response))\n",
    "\n",
    "    @step\n",
    "    async def critique_joke(self, ev: JokeEvent) -> StopEvent:\n",
    "        joke = ev.joke\n",
    "\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "        return StopEvent(result=str(response))\n",
    "\n",
    "\n",
    "w = JokeFlow(timeout=60, verbose=False)\n",
    "result = await w.run(topic=\"pirates\")\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e0fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core.workflow import StartEvent\n",
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "class MyCustomStartEvent(StartEvent):\n",
    "    a_string_field: str\n",
    "    a_path_to_somewhere: Path\n",
    "    an_index: VectorStoreIndex\n",
    "    an_llm: OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3bb47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeFlow(Workflow):\n",
    "    ...\n",
    "\n",
    "    @step\n",
    "    async def generate_joke_from_index(\n",
    "        self, ev: MyCustomStartEvent\n",
    "    ) -> JokeEvent:\n",
    "        # Build a query engine using the index and the llm from the start event\n",
    "        query_engine = ev.an_index.as_query_engine(llm=ev.an_llm)\n",
    "        topic = query_engine.query(\n",
    "            f\"What is the closest topic to {ev.a_string_field}\"\n",
    "        )\n",
    "        # Use the llm attached to the start event to instruct the model\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await ev.an_llm.acomplete(prompt)\n",
    "        # Dump the response on disk using the Path object from the event\n",
    "        ev.a_path_to_somewhere.write_text(str(response))\n",
    "        # Finally, pass the JokeEvent along\n",
    "        print(str(response))\n",
    "        return JokeEvent(joke=str(response))\n",
    "\n",
    "    @step\n",
    "    async def critique_joke(self, ev: JokeEvent) -> StopEvent:\n",
    "        joke = ev.joke\n",
    "\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "        response = await Settings.llm.acomplete(prompt)\n",
    "        return StopEvent(result=str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3343328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the computer go to therapy?\n",
      "\n",
      "Because it had too many unresolved issues!\n",
      "This joke employs a classic structure of humor that relies on wordplay and the anthropomorphism of technology. Let's break it down into its components for a thorough analysis and critique.\n",
      "\n",
      "### Structure and Setup\n",
      "\n",
      "1. **Setup**: \"Why did the computer go to therapy?\"\n",
      "   - This line sets up an expectation. It introduces a scenario that is inherently humorous because it anthropomorphizes a computer, attributing human-like emotions and behaviors (such as seeking therapy) to an inanimate object. This creates a cognitive dissonance that is often a source of humor.\n",
      "\n",
      "2. **Punchline**: \"Because it had too many unresolved issues!\"\n",
      "   - The punchline delivers the humor through a play on words. The term \"unresolved issues\" can refer to both psychological problems that a person might bring to therapy and technical problems that a computer might have, such as bugs or errors in its programming.\n",
      "\n",
      "### Analysis of Humor Elements\n",
      "\n",
      "1. **Wordplay**: The humor hinges on the double meaning of \"unresolved issues.\" This is a common comedic device known as a pun. The effectiveness of the pun relies on the audience's understanding of both contextsâ€”psychological and technical.\n",
      "\n",
      "2. **Anthropomorphism**: By attributing human characteristics (the need for therapy) to a computer, the joke creates a whimsical image that can elicit laughter. This technique is effective in making technology relatable and highlighting the absurdity of the situation.\n",
      "\n",
      "3. **Cognitive Dissonance**: The juxtaposition of a computer, a non-sentient object, engaging in a human activity (therapy) creates a humorous tension. The audience is led to momentarily suspend disbelief, which is a key element in many jokes.\n",
      "\n",
      "### Critique\n",
      "\n",
      "1. **Originality**: While the joke is clever, it is not particularly original. The concept of computers having \"issues\" is a well-trodden path in humor, especially in the context of technology. Many jokes and memes have explored similar themes, which may lessen its impact for audiences familiar with this type of humor.\n",
      "\n",
      "2. **Target Audience**: The joke may resonate more with audiences who have a basic understanding of both psychology and technology. For those who are less familiar with either field, the punchline may not land as effectively. This could limit its appeal to a broader audience.\n",
      "\n",
      "3. **Depth**: The joke is relatively simple and does not delve into deeper themes or complexities. While this is not inherently a flawâ€”many jokes thrive on simplicityâ€”it may not satisfy audiences looking for more nuanced or layered humor.\n",
      "\n",
      "4. **Cultural Relevance**: The joke taps into contemporary discussions about mental health and the normalization of therapy, which can be seen as a positive aspect. However, it also risks trivializing the serious nature of mental health issues by reducing them to a punchline. This could be viewed as insensitive, depending on the audience's experiences and perspectives.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Overall, the joke \"Why did the computer go to therapy? Because it had too many unresolved issues!\" is a light-hearted play on words that effectively uses anthropomorphism and cognitive dissonance to elicit humor. While it may not be groundbreaking in its originality, it is accessible and relatable to many, particularly those familiar with technology and mental health. However, its simplicity and potential insensitivity to mental health issues may limit its effectiveness and appeal in certain contexts.\n"
     ]
    }
   ],
   "source": [
    "custom_start_event = MyCustomStartEvent(\n",
    "    a_string_field=\"gumba\",\n",
    "    a_path_to_somewhere=Path(\"joke.txt\"),\n",
    "    an_index=index,\n",
    "    an_llm=Settings.llm,\n",
    ")\n",
    "w = JokeFlow(timeout=60, verbose=False)\n",
    "result = await w.run(start_event=custom_start_event)\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcd92b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the computer go to therapy?\n",
      "\n",
      "Because it had too many bytes of unresolved issues!\n",
      "joke_flow_recent.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import (\n",
    "    draw_all_possible_flows,\n",
    "    draw_most_recent_execution,\n",
    ")\n",
    "\n",
    "# Draw all\n",
    "# draw_all_possible_flows(JokeFlow, filename=\"joke_flow_all.html\")\n",
    "\n",
    "# Draw an execution\n",
    "w = JokeFlow()\n",
    "await w.run(start_event=custom_start_event)\n",
    "draw_most_recent_execution(w, filename=\"joke_flow_recent.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361ce3d",
   "metadata": {},
   "source": [
    "# Global management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a378be9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyEvent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mworkflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Context\n\u001b[1;32m      4\u001b[0m \u001b[38;5;129m@step\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m, ctx: Context, ev: \u001b[43mMyEvent\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StopEvent:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# retrieve from context\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# do something with context and event\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyEvent' is not defined"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "\n",
    "@step\n",
    "async def query(self, ctx: Context, ev: MyEvent) -> StopEvent:\n",
    "    # retrieve from context\n",
    "    query = await ctx.store.get(\"query\")\n",
    "\n",
    "    # do something with context and event\n",
    "    val = ...\n",
    "    result = ...\n",
    "\n",
    "    # store in context\n",
    "    await ctx.store.set(\"key\", val)\n",
    "\n",
    "    return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "399b5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator, field_serializer\n",
    "from typing import Union\n",
    "from pydantic.config import ConfigDict   # v2\n",
    "\n",
    "# This is a random object that we want to use in our state\n",
    "class MyRandomObject:\n",
    "    def __init__(self, name: str = \"default\"):\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "# This is our state model\n",
    "# NOTE: all fields must have defaults\n",
    "class MyState(BaseModel):\n",
    "    my_obj: MyRandomObject = Field(default_factory=MyRandomObject)\n",
    "    some_key: str = Field(default=\"some_value\")\n",
    "\n",
    "    model_config = ConfigDict(\n",
    "        arbitrary_types_allowed=True     # ðŸ‘‰ ì»¤ìŠ¤í…€ íƒ€ìž… í—ˆìš©\n",
    "    )\n",
    "\n",
    "    # # This is optional, but can be useful if you want to control the serialization of your state!\n",
    "\n",
    "    # @field_serializer(\"my_obj\", when_used=\"always\")\n",
    "    # def serialize_my_obj(self, my_obj: MyRandomObject) -> str:\n",
    "    #     return my_obj.name\n",
    "\n",
    "    # @field_validator(\"my_obj\", mode=\"before\")\n",
    "    # @classmethod\n",
    "    # def deserialize_my_obj(\n",
    "    #     cls, v: Union[str, MyRandomObject]\n",
    "    # ) -> MyRandomObject:\n",
    "    #     if isinstance(v, MyRandomObject):\n",
    "    #         return v\n",
    "    #     if isinstance(v, str):\n",
    "    #         return MyRandomObject(v)\n",
    "\n",
    "    #     raise ValueError(f\"Invalid type for my_obj: {type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e78c717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def start(self, ctx: Context[MyState], ev: StartEvent) -> StopEvent:\n",
    "        # Returns MyState directly\n",
    "        state = await ctx.store.get_state()\n",
    "        state.my_obj.name = \"new_name\"\n",
    "        await ctx.store.set_state(state)\n",
    "\n",
    "        # Can also access fields directly if needed\n",
    "        name = await ctx.store.get(\"my_obj.name\")\n",
    "        await ctx.store.set(\"my_obj.name\", \"newer_name\")\n",
    "\n",
    "        return StopEvent(result=\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4df855cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import step, Context, Event, Workflow\n",
    "\n",
    "\n",
    "class MyEvent(Event):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MyEventResult(Event):\n",
    "    result: str\n",
    "\n",
    "\n",
    "class GatherEvent(Event):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def dispatch_step(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> MyEvent | GatherEvent:\n",
    "        ctx.send_event(MyEvent())\n",
    "        ctx.send_event(MyEvent())\n",
    "\n",
    "        return GatherEvent()\n",
    "\n",
    "    @step\n",
    "    async def handle_my_event(self, ev: MyEvent) -> MyEventResult:\n",
    "        return MyEventResult(result=\"result\")\n",
    "\n",
    "    @step\n",
    "    async def gather(\n",
    "        self, ctx: Context, ev: GatherEvent | MyEventResult\n",
    "    ) -> StopEvent | None:\n",
    "        # wait for events to finish\n",
    "        events = ctx.collect_events(ev, [MyEventResult, MyEventResult])\n",
    "        if not events:\n",
    "            return None\n",
    "\n",
    "        return StopEvent(result=events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0393b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = MyWorkflow(...)\n",
    "\n",
    "handler = w.run(topic=\"Pirates\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    print(event)\n",
    "\n",
    "result = await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f01533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec6ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d70471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db77ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab572e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
